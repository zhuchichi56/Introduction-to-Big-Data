

# 大数据导论报告

###### 12012506朱赫 12012825 潘腾

### 1.Data exploration:

#### 1.1data statistics 

<img src="/Users/zhuhe/Library/Application Support/typora-user-images/image-20220530105901250.png" alt="image-20220530105901250" style="zoom:30%;" />

missing value 只在pm2.5 中出现

#分布





#### 1.2data visualization

#Y X 线性关系 



```python
sns.pairplot(engineer_set,vars = [ 'DEWP', 'TEMP', 'PRES','Iws','pm2.5'],diag_kind = 'kde')
```

<img src="/Users/zhuhe/Library/Application Support/typora-user-images/image-20220530134722241.png" alt="image-20220530134722241" style="zoom:33%;" />



对于一下几个主要的数值类型的数据：
1.DEWP 和 TEMP 存在多重共线性;
2.PRES 与DEWP, TEMP 有较明显的负相关
3.对于PM2.5的影响:
DEWP ,TEMP呈现左偏：

DewPoint（露点或露点温度是在固定气压之下，空气中所含的气态水达到饱和而凝结成液态水所需要降至的温度。在这温度时，凝结的水飘浮在空中称为雾、而沾在固体表面上时则称为露，因而得名“露点温度”。）

PRES 呈现右偏，lws有明显的负相关关系（风速越大,空气流动速率快，不易造成污染）

我们所建立的模型需要避免多重共线性，并且能够有效捕捉feature和label之间的关系，同时能够对异常值有一定的免疫效果，我们决定采用树系模型(Decision tree, RF, Boosting)





```python
sns.displot(engineer_set,y ='pm2.5',kind='hist',x= 'No', aspect=1.2)
sns.displot(engineer_set,y ='pm2.5',kind='hist',x= 'year', aspect=1.2)
sns.displot(engineer_set,y ='pm2.5',kind='hist',x= 'month', aspect=1.2)
sns.displot(engineer_set,y ='pm2.5',kind='hist',x= 'day', aspect=1.2)
sns.displot(engineer_set,y ='pm2.5',kind='hist',x= 'hour', aspect=1.2)
plt.show()
```

```python
sns.pairplot(engineer_set,vars = [ 'No', 'year', 'month', 'day', 'hour', 'pm2.5'],diag_kind = 'kde')
```

![image-20220530144438313](/Users/zhuhe/Library/Application Support/typora-user-images/image-20220530144438313.png)





```
#如何创建数据集呢？
#1。过5天，取一天，过五天取一天
#2。滑动窗口 这个一般用于上下文有明显的联系关系的; 不能随便打乱顺序
#3。机器学习本质上是通过结构feauter进行分类；，所以和顺序是无关；
#4。每年不同因素都可能有不同的影响吧，比如2010 年dew point高，但是其他不高
```



```
#增加特征:
```

text.drop(columns=['year', 'month', 'day', 'hour', 'No'], inplace=True)
测下这个











模型的选择：



## 四.模型CV的思路：



由之前的对比可得，我们选择各方面均衡稳定并且模型上限较高的XGBoosting作为核心模型。



##### 不需要调优的参数

- booster ： [default=gbtree]   决定类型 ，gbtree   :   树模型    gblinear :线性模型。

- objective : 

- - binary:logistic 用来二分类
  - multi:softmax 用来多分类
  - reg:linear 用来回归任务

- silent [default=0]:

- - 设为1 则不打印执行信息
  - 设为0打印信息

- nthread ：控制线程数目



##### 可以优化的参数

- max_depth: 决定树的最大深度，比较重要 常用值4-6 ，深度越深越容易过拟合。
- n_estimators:  构建多少颗数 ，树越多越容易过拟合。
- learning_rate/eta : 学习率 ，范围0-1 。默认值 为0.3 , 常用值0.01-0.2 
- subsample: 每次迭代用多少数据集 。
- colsample_bytree: 每次用多少特征 。可以控制过拟合。
- min_child_weight :树的最小权重 ，越小越容易过拟合。
- gamma：如果分裂能够使loss函数减小的值大于gamma，则这个节点才分裂。gamma设置了这个减小的最低阈值。如果gamma设置为0，表示只要使得loss函数减少，就分裂。
- alpha：l1正则，默认为0 
- lambda ：l2正则，默认为1



##### 调节的核心思路：根据复杂度和准确度的关系进行调节，如果复杂度上升但是测试准确度下降，则会有过拟合的风险;如果模型复杂度下降但是测试准确度下降，则会有欠拟合的风险。

##### 1.调节思路优先调节n_estimators，影响最大

```python

cv_params = {'n_estimators': [500, 1500, 3600, 7700, 10000]}
params = {'learning_rate': 0.1, 'n_estimators': 500, 'max_depth': 5, 'min_child_weight': 1, 'seed': 0,
          'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0, 'reg_alpha': 0, 'reg_lambda': 1}
model = xgb.XGBRegressor(**params)
optimized_GBM = GridSearchCV(estimator=model, param_grid=cv_params, scoring='r2', cv=5, verbose=1, n_jobs=4)
optimized_GBM.fit(X_train, Y_train)
```

###### #在没有调节参数之前的模型正确率:0.681234234

#400:0.6463348833232134

#500:0.6560019287478827

#700:0.6731691368596099

#1000:0.6995648035804803

#2000:0.7233678913589896

#2500  :0.7275989784547924

#3000  :0.7301169973268593

#5000：0.7352710164754139

#10000: 0.7350062402022233

#### 根据时间,计算资源和准确度的trade_off,  我们选择3000 作为最佳n_estimators(最佳值准确度是4000)

##### #方便起见，我门之后的所有模型测试均以n_estimators=默认进行。

##### 2.调节和booster相关参数

#max_depth

#min_child_weight

 #learning rate

```python
v_params = {'max_depth': [3, 4, 5, 6, 7], 'min_child_weight': [1, 2, 3]}
params = {'learning_rate': 0.1, 'n_estimators': 550, 'max_depth': 5, 'min_child_weight': 1, 'seed': 0,
          'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0, 'reg_alpha': 0, 'reg_lambda': 1}
cv_params = {'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]}
params = {'learning_rate': 0.1, 'n_estimators': 550, 'max_depth': 4, 'min_child_weight': 5, 'seed': 0,
          'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0, 'reg_alpha': 0, 'reg_lambda': 1}
         
cv_params = {'learning_rate': [0.01, 0.05, 0.07]}
params = {'learning_rate': 0.1, 'n_estimators': 550, 'max_depth': 4, 'min_child_weight': 5, 'seed': 0,
          'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0.1, 'reg_alpha': 1, 'reg_lambda': 1}
```

##### 我们最终选择n_estimators = 3000,其他参数默认的XGBoosting进行调节参数。



# 五.Bonus：

#### 一.滑动窗口+三分类问题：

##### 1.1总体思路：

###### 1.运用滑动窗口摄取3小时内'DEWP','TEMP','PRES','Iws','Is','Ir'特征的时间序列信息，将三数的中位数作为对应的最终值进行预测

###### 2.通过标准将回归问题转化成分类问题，在不损失模型意义的情况下，可以小幅度提升模型的准确性

```python
#化成三分类
def transform_three_kinds(x):
    if(x<=35) :return 'low'
    elif(x<=150): return 'polluting '
    else:return 'very high'
text['pm2.5'] =text.apply(lambda x: transform_three_kinds(x['pm2.5']),axis=1)
dict_PM2_5 = norm_to_float('pm2.5')
```

```python
#滑动窗口
sub_train_test =Train_set.loc[:,['DEWP','TEMP','PRES','Iws','Is','Ir']]
rolling_array = sub_train_test.rolling(3).mean()
new_Train_set = Train_set.copy()
for i in ['DEWP','TEMP','PRES','Iws','Is','Ir']:
    new_Train_set[i] = rolling_array[i]
```

##### 1.2分数表现：

```python
from xgboost import XGBClassifier,XGBRegressor
from sklearn.utils import shuffle

#shuffle True: 打乱顺序  select = "Regression" or other
def get_score(trainset,testset,select,shuffle=False):

    if(shuffle==True):
        trainset = shuffle(trainset)
        testset = shuffle(testset)
    x_train =trainset.astype("float")
    x_test =testset.astype("float")

    y_train =x_train.pop('pm2.5')
    y_test =x_test.pop('pm2.5')
    if(select=='Regression'):
        model = XGBRegressor()
    else:
        model = XGBClassifier()
    model.fit(x_train,y_train)
    predict = model.predict(x_test)
    score = model.score(x_test,y_test)
    return score,predict,y_test
```



<img src="/Users/zhuhe/Library/Application Support/typora-user-images/image-20220530203705006.png" alt="image-20220530203705006" style="zoom:50%;" />

###### 在未经其他优化的情况下，已经达到75%的测试正确率，模型表达较优秀。

##### 1.3模型反思:

###### 1.3.1若去除滑动窗口方法，对比试验下，测试准确率相差不大。原因解释：分类数较小，无法体现时间序列的优越性.

###### 1.3.2 延伸到滑动窗口+回归预测问题:

###### 参照思路，进行滑动窗口与传统回归问题结合；

###### 用滑动窗口提取特征作为new_feature 进行训练得分如下:

<img src="/Users/zhuhe/Library/Application Support/typora-user-images/image-20220530215517502.png" alt="image-20220530215517502" style="zoom:50%;" />



###### 模型表现：相较与传统模型，有0.01百分点的上升。

#### 二.将回归问题变成多分类问题：

pm2.5 的指标在[0,1000]的范围内，上面模型我们将PM2.5分为3类,受此启发，我们不妨将pm2.5 每10个或者每20个划分出区间，生成(0,20],(20,40]......等等接近50个区间，将模型化成一个50分类问题。此划分方法可以得到良好的解释：相似的区间内pm2.5 所对应的各项指标也会近似，而且区间中值能够很好的表征空气质量。

##### 思路:将pm2.5 按照区间为20进行划分，(0,20]  (10,20]  (20,30]   (30,40],将区间的中值作为label；

```python

parameter = 20
new_text['pm2.5'] = new_text['pm2.5'].astype('int')
bins =range(0,len(new_text['pm2.5']),parameter )
pm25_Cut = pd.cut(new_text['pm2.5'],bins)
new_text['Categories'] = pm25_Cut
element = pm25_Cut.unique()
temp = list(element.sort_values())
start = int(parameter/2)
mid_region = range(start,1000,20)
list(mid_region)

dictonary = dict(zip(temp,mid_region))
new_text['Categories']  = new_text.apply(lambda x: dictonary.get(x.Categories),axis=1 )
```

```python
from xgboost import XGBClassifier,XGBRegressor
from sklearn.preprocessing import LabelEncoder
#XGBoosting
model = XGBClassifier(
    objective='reg:linear',
    colsample_bytree=0.3,
    learning_rate=0.1,
    max_depth=5,
    n_estimators=3000,
    alpha=10
)
le1 = LabelEncoder()
le1.fit(Y_train)
le2 = LabelEncoder()
le2.fit(Y_test)

y_train  =le1.transform(Y_train)
y_test =le2.transform(Y_test)
model.fit(X_train,y_train)
score = model.score(X_test,y_test)
import xgboost as xgb
xgb.plot_importance(model)
```

<img src="/Users/zhuhe/Library/Application Support/typora-user-images/image-20220530225653785.png" alt="image-20220530225653785" style="zoom:50%;" />

```python
def judge(diff):
   if(diff<=20 and diff>=-20):
       return True
   return False
a =diff.map(lambda x:judge(x))
number =0
for i in a:
    if(i==True):
        number+=1
number/len(a)

```

##### 最后和实际结果重合度较好；考虑到650和670的空气质量没有本质不同，因此我们设立可接受阈值为[-10，10].

##### 最终的模型可接受区间准确度在0.7附近;



#### 三.将LSTM 用于预测准确率：

##### 用LSTM进行捕捉时间序列进行预测：

```python
model = keras.Sequential()
model.add(layers.LSTM(32, input_shape=(120, 11), return_sequences=True)) 
model.add(layers.LSTM(32, return_sequences=True)) 
model.add(layers.LSTM(32)) 
model.add(layers.Dense(1))
model.summary()
```

```python
model.evaluate(test_x, test_y, verbose=0)
```































#### 