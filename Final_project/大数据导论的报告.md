# 大数据导论报告

###### 12012506朱赫 12012825 潘腾

## 一.Data statistics & Data visualization：

#### 1.1 我们首先将pm2.5 <100, pm2.5>250, pm2.5>500 ,pm2.5>750依次作数据探索。

得到如下数据：

<img src="/Users/zhuhe/Library/Application Support/typora-user-images/image-20220601015356284.png" alt="image-20220601015356284" style="zoom:50%;" />



<img src="/Users/zhuhe/Library/Application Support/typora-user-images/image-20220601015548658.png" alt="image-20220601015548658" style="zoom:50%;" />

######                                  																	 pm2.5>500

<img src="/Users/zhuhe/Library/Application Support/typora-user-images/image-20220601015638339.png" alt="image-20220601015638339" style="zoom:50%;" />

###### 																									pm2.5>250

<img src="/Users/zhuhe/Library/Application Support/typora-user-images/image-20220601015703820.png" alt="image-20220601015703820" style="zoom:50%;" />

###### 																									pm2.5<100

我们大概能猜想到到，在TEMP 气温较高，IWS累计风速越大，PRES气压越低，IR降雨量更多，露点在0附近的时候空气质量较好。

###### 因此我们猜测月份和季节是能够对预测产生一定影响的

#### 1.2 分析各变量和PM2.5之间的关系

```python
import seaborn as sns
sns.set(style="ticks", color_codes=True)
palette = sns.xkcd_palette(['dark blue', 'dark green', 'gold', 'orange'])
sns.pairplot(text,diag_kind = 'kde')
plt.show()
```

<img src="/Users/zhuhe/Library/Application Support/typora-user-images/image-20220601020223805.png" alt="image-20220601020223805" style="zoom:33%;" />

##### 同时用Examine函数仔细探索关系：

```python
def Examine(features):
    features_condition = text.groupby([features])
    print(features_condition.value_counts())
    value_of_feature = features_condition['Ir'].mean()
    value_of_feature.plot()
```



<img src="/Users/zhuhe/Library/Application Support/typora-user-images/image-20220601020504342.png" alt="image-20220601020504342" style="zoom:33%;" />

###### 																								 风向和pm2.5的关系

结论：1.可以看出cv风向对pm2.5 增长贡献最大。

​            2.当北风出现时，pm2.5 浓度显著下降





<img src="/Users/zhuhe/Library/Application Support/typora-user-images/image-20220601020614479.png" alt="image-20220601020614479" style="zoom:33%;" />

###### 																								 压力和pm2.5的关系

结论：可以看出在低气压<990和高气压>1030的情况下,pm2.5较低，[990,1030]之间，排除异常之后，相关性较小。

<img src="/Users/zhuhe/Library/Application Support/typora-user-images/image-20220601020643678.png" alt="image-20220601020643678" style="zoom:33%;" />

###### 																								 露点和pm2.5的关系

结论：可以看出露点和pm2.5有较强的线性关系。

<img src="/Users/zhuhe/Library/Application Support/typora-user-images/image-20220601020746639.png" alt="image-20220601020746639" style="zoom:33%;" />

###### 																								 温度和pm2.5的关系

结论：在排除异常之后，呈现左偏趋势，温度较低的时候pm2.5普遍偏高，猜测可能和季节是有关系的。春夏秋初的空气质量是较好的。

<img src="/Users/zhuhe/Library/Application Support/typora-user-images/image-20220601020902543.png" alt="image-20220601020902543" style="zoom:33%;" />

###### 																								 累计风速和pm2.5的关系

结论：可以看出累计风速和pm2.5有较强的线性关系。当风速较大的时候，能够有效加速空气的净化速度。

<img src="/Users/zhuhe/Library/Application Support/typora-user-images/image-20220601020922532.png" alt="image-20220601020922532" style="zoom:33%;" />

###### 																								 累计降雪和pm2.5的关系

结论：下雪出现在冬季，由图可得季节性影响显著。

<img src="/Users/zhuhe/Library/Application Support/typora-user-images/image-20220601020942814.png" alt="image-20220601020942814" style="zoom:33%;" />

###### 																								 累计降雨和pm2.5的关系

结论：可以看出累计降雨和pm2.5有较强的线性关系。当累计降雨较大的时候，能够有效加速空气的净化速度。

<img src="/Users/zhuhe/Library/Application Support/typora-user-images/image-20220601023401131.png" alt="image-20220601023401131" style="zoom:33%;" />

###### 																								 月份和pm2.5的关系



结论：可以看出月份和pm2.5有一定关系。表现秋冬季节空气质量变差。夏季空气质量也会明显变差。

<img src="/Users/zhuhe/Library/Application Support/typora-user-images/image-20220601023514762.png" alt="image-20220601023514762" style="zoom:33%;" />

###### 																								 小时和pm2.5的关系

结论：可以看出小时和pm2.5有较强的关系。一天内不同的时间段可能会对产生一定影响。



![image-20220601022819095](/Users/zhuhe/Library/Application Support/typora-user-images/image-20220601022819095.png)

![image-20220601022904933](/Users/zhuhe/Library/Application Support/typora-user-images/image-20220601022904933.png)

##### 再结合热力图可以得出，与预测pm2.5强相关的变量，大致是

#### 累计降雨，季节因素，累计风速，露点，风向，月份，小时。



#### 1.3 分析其他变量之间的关系

```
sns.pairplot(engineer_set,vars = [ 'DEWP', 'TEMP', 'PRES','Iws','pm2.5'],diag_kind = 'kde')
```

<img src="/Users/zhuhe/Library/Application Support/typora-user-images/image-20220601023932237.png" alt="image-20220601023932237" style="zoom:25%;" />



##### 结论：

##### 1.DEWP 和 TEMP 存在多重共线性;
2.PRES 与DEWP, TEMP 有较明显的负相关



#### 1.4 数据总结：

通过数据探索，我们发现：

##### 1.和预测pm2.5强相关的变量，大致是累计降雨，季节因素，累计风速，露点，风向，月份，小时。

##### 2.DEWP 和 TEMP 存在多重共线性，PRES 与DEWP, TEMP 有较明显的负相关

##### 3.冬季暖冬效应：在每年的11月份，到来年的3月份，pm2.5的平均水平会更高，而且在期间过渡时期会产生显著变化。

<img src="/Users/zhuhe/Library/Application Support/typora-user-images/image-20220601035830416.png" alt="image-20220601035830416" style="zoom:50%;" />

######                                               														从9月进入10月

<img src="/Users/zhuhe/Library/Application Support/typora-user-images/image-20220601035905544.png" alt="image-20220601035905544" style="zoom:50%;" />

###### 																										从2月进入4月

##### 同时观察小时变化：在夜间pm2.5水平较高，在白天pm2.5水平较低。这可能和气温转冷转暖由关系。气温转冷->煤炭燃烧->pm2.5 升高

##### 4.北风效应：北风刮来，pm2.5下降显著。

<img src="/Users/zhuhe/Library/Application Support/typora-user-images/image-20220601040412320.png" alt="image-20220601040412320" style="zoom:50%;" />

##### 

##### 5.湿度效应：DEWP越高，PM2.5越高。空气中水分子越多，越容易凝结小分子，加速pm2.5的产生。DEWP是湿度的直接表征。

##### 6.人流活动效应：在早高峰和晚高峰的时候，汽车流动所造成的空气污染可能造成pm2.5的提升

##### 7.APEC蓝效应：APEC时期政府强制措施减排，导致该段时间PM2.5下降显著。



#### 1.5数据生成：

##### 我们将用此些变量生成更多有意义的变量：

##### 1.北风强度值：

###### 将北风单独作为一列，其值等于西北风IWS或东北风IWS的值；其他则是0；

##### 2.按照特殊标准划分的季节：

###### 11——2 冬 3——5春 6——8夏 9——10秋

###### 对应冬季供暖季的到来

##### 3.划分时间段：

###### 8----10早高峰  10----17工作时间  18----20晚高峰   20---次日8 休息时间（在家时间）

##### 4.划分工作日和节假日：

###### week weekend holiday 

##### 5.湿度风速差（a露点（湿度）-b北风强度值） PCA（尝试捕捉关系）

###### 湿度越高，北风强度值越低，pm2.5必然会越高








## 二. Data preprocessing

### 2.1 missing value
采用5种方式处理缺失值。分别为：直接删除，均值填充，中位数填充，Knn填充，随机森林填充

（下图对应socre自上向下，对应横坐标分别为1，2，3，4，5）。

```python
t_s= Test_set.dropna()
train_s =Train_set.dropna()
Scores=[]
dropna_scores = get_score(train_s,t_s)
average_scores = get_score(average_fill_in,t_s)
median_scores = get_score(median_fill_in,t_s)
missKnn_scores = get_score(missKnn_fill_in,t_s)
foreset_scores = get_score(missforest_fill_in,t_s)
Scores.append(dropna_scores)
Scores.append(average_scores)
Scores.append(median_scores)
Scores.append(missKnn_scores)
Scores.append(foreset_scores)
```
结果：

<img src="/Users/zhuhe/Library/Application Support/typora-user-images/image-20220601061211940.png" alt="image-20220601061211940" style="zoom:50%;" />

<img src="/Users/zhuhe/Library/Application Support/typora-user-images/image-20220601061414984.png" alt="image-20220601061414984" style="zoom:50%;" />

##### 可以看出均值填充效果最好。(随机森林填充和knn填充并没有进行调节参数)




### 2.2 outlier detection

##### 2.2.1需要检测的量&异常值检测函数：：

###### DEWP、TEMP、PRES、Iws、pm2.5

```python
import seaborn as sns
def outliers_proc(data, col_name, top_limit,bottom_limit,scale=3):
        
    def box_plot_outliers(data_ser, box_scale):
        iqr = box_scale * (data_ser.quantile(top_limit) - data_ser.quantile(bottom_limit))
        val_low = data_ser.quantile(bottom_limit) - iqr
        val_up = data_ser.quantile(top_limit) + iqr
        rule_low = (data_ser < val_low)
        rule_up = (data_ser > val_up)
        return (rule_low,rule_up),(val_low,val_up)
    
    data_n = data.copy()
    data_serier = data_n[col_name]
    rule, value = box_plot_outliers(data_serier,box_scale=scale)
    index = np.arange(data_serier.shape[0])[rule[0]|rule[1]]

    fig, ax = plt.subplots(1,2, figsize=(10,7))
    sns.boxplot(y=data[col_name],data=data,palette="Set1",ax=ax[0])
    sns.boxplot(y=data_n[col_name],data=data_n,palette="Set1",ax=ax[1])
    return index
```

##### 2.2.2检测DEWP、TEMP、PRES都没有异常值。

```python
out_index = outliers_proc(Train_set,'TEMP',0.75,0.25,scale=3)
out_index = outliers_proc(Train_set,'DEWP',0.75,0.25,scale=3)
out_index = outliers_proc(Train_set,'PRES',0.75,0.25,scale=3)

print("outlier的数量%d\n"%len(out_index)) 
print("outlier的索引:")
print(out_index)
```

###### 检测Iws，发现异常值有2579个。

![](https://codimd.s3.shivering-isles.com/demo/uploads/d7455c72-4cbf-4e24-97e0-f9eebc4e700c.png)

```
out_index = outliers_proc(Train_set,'Iws',0.75,0.25,scale=3)
print("outlier的数量%d\n"%len(out_index)) 
print("outlier的索引:")
print(out_index)
```
![](https://codimd.s3.shivering-isles.com/demo/uploads/e9d02ab9-069e-4404-a2c2-1f7adf02e300.png)

结合Iws的含义为累计风速推测，此情况为遇到大风天气，导致风速远大于均值。故不能算作异常。另外想到可以限制异常值的标准更加严格，改为上界为0.95,下界为0.05。检测出异常值有16个

```
out_index = outliers_proc(Train_set,'Iws',0.95,0.05,scale=3)
```
![](https://codimd.s3.shivering-isles.com/demo/uploads/e4375c0b-e9cb-426a-b25d-f22a5b5efa39.png)



##### 2.2.3检测pm2.5，检测出异常值有197个。均具有实际意义，故不舍弃。

![](https://codimd.s3.shivering-isles.com/demo/uploads/ee4a6848-19cd-4b5f-8740-36b713494d03.png)





### 2.3 nomalization

###### 我们使用树模型，而树模型不用标准化。因为树模型是找最佳分裂点，是否对数据进行归一化，不影响最佳分裂点的计算。此外，树模型是不进行梯度下降的，阶跃点是不可导的，并且求导没意义，也就不需要归一化。




### 2.4 删除特征:
###### 不删除时:

![](https://codimd.s3.shivering-isles.com/demo/uploads/1d39c648-aa41-42c9-a09b-742bb9a50b4e.png)
分别删除'No'列和'hour'列,score分别约为0.62和0.66。
![](https://codimd.s3.shivering-isles.com/demo/uploads/8766eb6f-ed8f-43a2-bdb9-0971349b7cd1.png)
![](https://codimd.s3.shivering-isles.com/demo/uploads/b7c03c41-8850-4e0b-ba1b-ac64f4114087.png)

##### 可以看出，不能删除特征，删除特征会使准确率下降。



## 三 feature selection

##### 3.1 使用RandomForestRegressor

```python
from sklearn.ensemble import RandomForestRegressor
trees = RandomForestRegressor(n_estimators=150,random_state=0)
trees.fit(X_train,Y_train)
score = trees.score(X_test,Y_test)
print(score)
pd.DataFrame(
    {'feature': list(X_train.columns),'importance': trees.feature_importances_}).sort_values('importance', ascending = False)
```
结果：

<img src="/Users/zhuhe/Library/Application Support/typora-user-images/image-20220601060026042.png" alt="image-20220601060026042" style="zoom:33%;" />



##### 3.2 使用XGBoosting

```python
#XGBoosting
model = XGBRegressor()
model.fit(X_train,Y_train)
score = model.score(X_test,Y_test)
import xgboost as xgb
xgb.plot_importance(model)
```
<img src="/Users/zhuhe/Library/Application Support/typora-user-images/image-20220601060245895.png" alt="image-20220601060245895" style="zoom:50%;" />



##### 3.3 使用皮尔逊相关系数
```python
from scipy.stats import pearsonr
from sklearn.feature_selection import SelectKBest
# Y_predict=model.predict(X_test)
func_pearson = lambda X,y: list (np.array ([pearsonr (x,y) for x in X.T]).T)
X_filtered = SelectKBest(func_pearson, k = 5).fit_transform(X_train, Y_train)
skb=SelectKBest(func_pearson,k=8)
skb.fit(X_train,Y_train)
index =skb.get_support(indices=True)
index = pd.Series(index)
X_train.iloc[:,index]
```
<img src="/Users/zhuhe/Library/Application Support/typora-user-images/image-20220601060408597.png" alt="image-20220601060408597" style="zoom:50%;" />


##### 3.4 使用互信息与最大信息系数（mic）
```python
from minepy import MINE
from sklearn.feature_selection import SelectKBest
#由于MINE的设计不是函数式的，定义mic方法将其为函数式的，返回一个二元组，二元组的第2项设置成固定的P值0.5
def mic(x, y):
    m = MINE()
    m.compute_score(x, y)
    return (m.mic(), 0.5)
# 选择K个最好的特征，返回特征选择后的数据
func_mic = lambda X,y: list(np.array([mic(x,y) for x in X.T]).T)
X_filtered_MIC = SelectKBest(func_mic,k = 8).fit(X_train.values,Y_train)
index =X_filtered_MIC.get_support(indices=True)
index = pd.Series(index)
X_train.iloc[:,index]
```
<img src="/Users/zhuhe/Library/Application Support/typora-user-images/image-20220601060445766.png" alt="image-20220601060445766" style="zoom:50%;" />



### 总结
##### 根据以上数据总结出，最用要的几个特征是：

##### 1.DEWP 2.Iws 3.N_E(北风强度值) 4.diff（湿度风速差） 5.composition(湿度风速差pca提取) 

##### 由此可见，生成的几种特征对模型提高正确率由积极作用。特征具体解释见1.4 1.5





## 四 Model Selection & Model Improvement

##### 我们所建立的模型需要：

##### 1.避免多重共线性

##### 2.能够有效捕捉feature和label之间的关系

##### 3.能够对异常值有一定的免疫效果，避免归一化

##### 我们决定采用树系模型(Decision tree, RF, Boosting)

### 4.1 model selection

使用七种模型，根据score，选择最好的XGBoost
```python
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import LinearSVR
from sklearn.linear_model import Lasso
from xgboost import XGBRegressor


def model_evaluation(model,name,X_train,Y_train,X_test,Y_test):
    model.fit(X_train,Y_train)
    score= model.score(X_test,Y_test)
    print("使用%s模型: 准确率%.5f:"%(name,score))

model_evaluation(LinearRegression(),'线性回归',X_train,Y_train,X_test,Y_test)
model_evaluation(Ridge(),'岭回归',X_train,Y_train,X_test,Y_test)
model_evaluation(Lasso(),'LASSO',X_train,Y_train,X_test,Y_test)
model_evaluation(LinearSVR(),'SVM回归',X_train,Y_train,X_test,Y_test)
model_evaluation(DecisionTreeRegressor(),'决策树回归',X_train,Y_train,X_test,Y_test)
model_evaluation(RandomForestRegressor(),'随机森林回归',X_train,Y_train,X_test,Y_test)
model_evaluation(XGBRegressor(),'XGboost',X_train,Y_train,X_test,Y_test)
```

![](https://codimd.s3.shivering-isles.com/demo/uploads/e5163b7d-4eb4-42a1-ae85-7f2da1674146.png)


### 分析：

#### LinearRegression,Ridge,LASSO
- 三者都在sklearn.linear_model下，适合线性模型的分类，而比那里和pm2.5并非线性的,所以效果不好。

#### SVM,decision tree
最原始用作处理分类模型,虽然也可以用作回归,但效果不好。尤其是LinearSVR处理非线性模型，效果更佳不好，为负数。

#### XGBoost
- XGBoost的全称是eXtreme Gradient Boosting，是经过优化的分布式梯度提升库。
- 列抽样： XGBoost 借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算。
- 正则化： XGBoost 在目标函数中加入了正则项，用于控制模型的复杂度。
- XGBoost 在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。

总结:XGBoost 算法对于中低维数据具有很好的处理速度和精度，缺点是对于例如大规模图像物体识别。但本次project数据维度较低，适合使用XGBoost




### 4.2 Model improvement

由之前的对比可得，我们选择各方面均衡稳定并且模型上限较高的XGBoosting作为核心模型。

##### 不需要调优的参数

- booster ： [default=gbtree]   决定类型，gbtree   :   树模型    gblinear :线性模型。

- objective : 

- - binary:logistic 用来二分类
  - multi:softmax 用来多分类
  - reg:linear 用来回归任务

- silent [default=0]:

- - 设为1 则不打印执行信息
  - 设为0打印信息

- nthread ：控制线程数目

##### 可以优化的参数

- max_depth: 决定树的最大深度，比较重要 常用值4-6 ，深度越深越容易过拟合。
- n_estimators:  构建多少颗数 ，树越多越容易过拟合。
- learning_rate/eta : 学习率 ，范围0-1 。默认值 为0.3 , 常用值0.01-0.2 
- subsample: 每次迭代用多少数据集 。
- colsample_bytree: 每次用多少特征 。可以控制过拟合。
- min_child_weight :树的最小权重 ，越小越容易过拟合。
- gamma：如果分裂能够使loss函数减小的值大于gamma，则这个节点才分裂。gamma设置了这个减小的最低阈值。如果gamma设置为0，表示只要使得loss函数减少，就分裂。
- alpha：l1正则，默认为0 
- lambda ：l2正则，默认为1



##### 调节的核心思路：根据复杂度和准确度的关系进行调节，如果复杂度上升但是测试准确度下降，则会有过拟合的风险;如果模型复杂度下降但是测试准确度下降，则会有欠拟合的风险。

##### 1.调节思路优先调节n_estimators，影响最大

```python
cv_params = {'n_estimators': [500, 1500, 3600, 7700, 10000]}
params = {'learning_rate': 0.1, 'n_estimators': 500, 'max_depth': 5, 'min_child_weight': 1, 'seed': 0,
          'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0, 'reg_alpha': 0, 'reg_lambda': 1}
model = xgb.XGBRegressor(**params)
optimized_GBM = GridSearchCV(estimator=model, param_grid=cv_params, scoring='r2', cv=5, verbose=1, n_jobs=4)
optimized_GBM.fit(X_train, Y_train)
```

###### #在没有调节参数之前的模型正确率:0.681234234

#400:0.6463348833232134

#500:0.6560019287478827

#700:0.6731691368596099

#1000:0.6995648035804803

#2000:0.7233678913589896

#2500  :0.7275989784547924

#3000  :0.7301169973268593

#5000：0.7352710164754139

#10000: 0.7350062402022233

#### 根据时间,计算资源和准确度的trade_off,  我们选择3000 作为最佳n_estimators(最佳值准确度是4000)

##### #方便起见，我门之后的所有模型测试均以n_estimators=默认进行。

##### 2.调节和booster相关参数

#max_depth

#min_child_weight

 #learning rate

```python
v_params = {'max_depth': [3, 4, 5, 6, 7], 'min_child_weight': [1, 2, 3]}
params = {'learning_rate': 0.1, 'n_estimators': 550, 'max_depth': 5, 'min_child_weight': 1, 'seed': 0,
          'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0, 'reg_alpha': 0, 'reg_lambda': 1}
cv_params = {'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]}
params = {'learning_rate': 0.1, 'n_estimators': 550, 'max_depth': 4, 'min_child_weight': 5, 'seed': 0,
          'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0, 'reg_alpha': 0, 'reg_lambda': 1}
         
cv_params = {'learning_rate': [0.01, 0.05, 0.07]}
params = {'learning_rate': 0.1, 'n_estimators': 550, 'max_depth': 4, 'min_child_weight': 5, 'seed': 0,
          'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0.1, 'reg_alpha': 1, 'reg_lambda': 1}
```

##### 我们最终选择n_estimators = 3000,其他参数默认的XGBoosting进行调节参数。





## 五 模型性能评估：

```python
#模型评估（准确率、查准率、查全率、F1值）：
from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score  # 导入模型评估工具包

def model_evaluation(model,Y_test,Y_test_pred):
    print("使用%s模型:"%model)
    print("----------------验证集性能评估--------------------")
    print("验证集准确率: {:.2f}%".format(accuracy_score(Y_test, Y_test_pred) * 100))
    print("验证集查准率: {:.2f}%".format(precision_score(Y_test, Y_test_pred,average='micro') * 100))  # 打印验证集查准率
    print("验证集查全率: {:.2f}%".format(recall_score(Y_test, Y_test_pred,average='micro') * 100))  # 打印验证集查全率
    print("验证集F1值: {:.2f}%".format(f1_score(Y_test, Y_test_pred,average='micro') * 100))  # 打印验证集F1值
```



###### 对于回归模型，模型的R^2 系数如下：

<img src="/Users/zhuhe/Library/Application Support/typora-user-images/image-20220601062712293.png" alt="image-20220601062712293" style="zoom:50%;" />

###### 对于分类模型，模型的正确率如下：

<img src="/Users/zhuhe/Library/Application Support/typora-user-images/image-20220601063652076.png" alt="image-20220601063652076" style="zoom:50%;" />

## 五 Bonus：

#### 1.滑动窗口+三分类问题：

##### 1.1总体思路：

###### 1.运用滑动窗口摄取3小时内'DEWP','TEMP','PRES','Iws','Is','Ir'特征的时间序列信息，将三数的中位数作为对应的最终值进行预测

###### 2.通过标准将回归问题转化成分类问题，在不损失模型意义的情况下，可以小幅度提升模型的准确性

```python
#化成三分类
def transform_three_kinds(x):
    if(x<=35) :return 'low'
    elif(x<=150): return 'polluting '
    else:return 'very high'
text['pm2.5'] =text.apply(lambda x: transform_three_kinds(x['pm2.5']),axis=1)
dict_PM2_5 = norm_to_float('pm2.5')
```

```python
#滑动窗口
sub_train_test =Train_set.loc[:,['DEWP','TEMP','PRES','Iws','Is','Ir']]
rolling_array = sub_train_test.rolling(3).mean()
new_Train_set = Train_set.copy()
for i in ['DEWP','TEMP','PRES','Iws','Is','Ir']:
    new_Train_set[i] = rolling_array[i]
```

##### 1.2分数表现：

```python
from xgboost import XGBClassifier,XGBRegressor
from sklearn.utils import shuffle

#shuffle True: 打乱顺序  select = "Regression" or other
def get_score(trainset,testset,select,shuffle=False):

    if(shuffle==True):
        trainset = shuffle(trainset)
        testset = shuffle(testset)
    x_train =trainset.astype("float")
    x_test =testset.astype("float")

    y_train =x_train.pop('pm2.5')
    y_test =x_test.pop('pm2.5')
    if(select=='Regression'):
        model = XGBRegressor()
    else:
        model = XGBClassifier()
    model.fit(x_train,y_train)
    predict = model.predict(x_test)
    score = model.score(x_test,y_test)
    return score,predict,y_test
```

![image-20220601063646136](/Users/zhuhe/Library/Application Support/typora-user-images/image-20220601063646136.png)


###### 在未经其他优化的情况下，已经达到75%的测试正确率，模型表达较优秀。

##### 1.3模型反思:

###### 1.3.1若去除滑动窗口方法，对比试验下，测试准确率相差不大。原因解释：分类数较小，无法体现时间序列的优越性.

###### 1.3.2 延伸到滑动窗口+回归预测问题:

###### 参照思路，进行滑动窗口与传统回归问题结合；

###### 用滑动窗口提取特征作为new_feature 进行训练得分如下:

![](https://codimd.s3.shivering-isles.com/demo/uploads/cd8048bd-d5a9-45cf-95f5-cd89ba11bd08.png)


###### 模型表现：相较与传统模型，有0.01百分点的上升。

#### 2.将回归问题变成多分类问题：

pm2.5 的指标在[0,1000]的范围内，上面模型我们将PM2.5分为3类,受此启发，我们不妨将pm2.5 每10个或者每20个划分出区间，生成(0,20],(20,40]......等等接近50个区间，将模型化成一个50分类问题。此划分方法可以得到良好的解释：相似的区间内pm2.5 所对应的各项指标也会近似，而且区间中值能够很好的表征空气质量。

##### 思路:将pm2.5 按照区间为20进行划分，(0,20]  (10,20]  (20,30]   (30,40],将区间的中值作为label；

```python

parameter = 20
new_text['pm2.5'] = new_text['pm2.5'].astype('int')
bins =range(0,len(new_text['pm2.5']),parameter )
pm25_Cut = pd.cut(new_text['pm2.5'],bins)
new_text['Categories'] = pm25_Cut
element = pm25_Cut.unique()
temp = list(element.sort_values())
start = int(parameter/2)
mid_region = range(start,1000,20)
list(mid_region)

dictonary = dict(zip(temp,mid_region))
new_text['Categories']  = new_text.apply(lambda x: dictonary.get(x.Categories),axis=1 )
```

```python
from xgboost import XGBClassifier,XGBRegressor
from sklearn.preprocessing import LabelEncoder
#XGBoosting
model = XGBClassifier(
    objective='reg:linear',
    colsample_bytree=0.3,
    learning_rate=0.1,
    max_depth=5,
    n_estimators=3000,
    alpha=10
)
le1 = LabelEncoder()
le1.fit(Y_train)
le2 = LabelEncoder()
le2.fit(Y_test)

y_train  =le1.transform(Y_train)
y_test =le2.transform(Y_test)
model.fit(X_train,y_train)
score = model.score(X_test,y_test)
import xgboost as xgb
xgb.plot_importance(model)
```

![](https://codimd.s3.shivering-isles.com/demo/uploads/3cd6e6ad-2449-480d-8bc6-2a5026c3a285.png)


```python
def judge(diff):
   if(diff<=20 and diff>=-20):
       return True
   return False
a =diff.map(lambda x:judge(x))
number =0
for i in a:
    if(i==True):
        number+=1
number/len(a)

```

##### 最后和实际结果重合度较好；考虑到650和670的空气质量没有本质不同，因此我们设立可接受阈值为[-10，10].

##### 最终的模型可接受区间准确度在0.7附近;



#### 3.将LSTM 用于预测准确率：

##### 用LSTM进行捕捉时间序列进行预测：

```python
model = keras.Sequential()
model.add(layers.LSTM(32, input_shape=(120, 11), return_sequences=True)) 
model.add(layers.LSTM(32, return_sequences=True)) 
model.add(layers.LSTM(32)) 
model.add(layers.Dense(1))
model.summary()
```

```python
model.evaluate(test_x, test_y, verbose=0)
```

## 六 总结

#### 1.pm2.5的变化主要与北风强度，空气湿度，煤炭使用(冬季夜晚)，和人流移动（车辆移动减排）有关系

###### 以上的结论与北京的地理位置和华北的能源消耗结构是密不可分的。北京位于华北平原的西北角，其西边是太行山脉，北边是燕山山脉。北京以北的污染工业较少，所以北风带来的是相对洁净的空气。然而，北京从正东方向顺时针到西南方向，广泛地分布着消耗大量煤炭和化石燃料的重工业企业。东南风与西南风带来的污染物会在太行山和燕山的山脚下聚集。北京以东 152 公里的唐山市，以 钢铁工业著称，2012 年大约消耗 1 亿吨煤炭。北京以南的石家庄市和保定市的 煤炭消耗量同样巨大。2012 年华北平原的总煤炭消耗量为 10 亿吨，约占全国消耗量的四分之一，与之形成鲜明对比的是，华北平原只占全国土地面积的 5.6%。

#### 2.新生成的向量为：

##### 1.北风强度值：

###### 将北风单独作为一列，其值等于西北风IWS或东北风IWS的值；其他则是0；

##### 2.按照特殊标准划分的季节：

###### 11——2 冬 3——5春 6——8夏 9——10秋

###### 对应冬季供暖季的到来

##### 3.划分时间段：

###### 8----10早高峰  10----17工作时间  18----20晚高峰   20---次日8 休息时间（在家时间）

##### 4.划分工作日和节假日：

###### week weekend holiday 

##### 5.湿度风速差（a露点（湿度）-b北风强度值） PCA（尝试捕捉关系）

###### 湿度越高，北风强度值越低，pm2.5必然会越高

##### 经过模型验证和特征筛选，缺失对准确率的提升产生一定帮助

#### 3.将回归问题转化为分类问题是本文的一次创新尝试，在可接受范围内仍能达到不错的结果。

#### 4.还有许多前沿技术（transformer），新的特征等待发掘

