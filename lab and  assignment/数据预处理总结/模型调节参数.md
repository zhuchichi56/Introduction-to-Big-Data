决策树处理回归问题的原理：https://blog.csdn.net/Albert201605/article/details/81865261

根据feature 不断的划分空间，然后最后输出每一个空间对应的值（可能是平均数）

##### 从0实现一个决策树； 从0实现一个随机森林回归；需要用统计学习方法和网课重新复习一下决策树和随机森林算法









#### 核心原则：

① 基于**泛化误差与模型复杂度的关系**来进行调参；

② 根据**对模型的影响程度**，由大到小对参数排序，并确定哪些参数会使模型复杂度减小，哪些会增大；

③ 依次选择合适的参数，通过绘制学习曲线或网格搜索的方法调参，直到找到最大准确得分。



交叉验证常用算法步骤：

```
param_test2 = {"max_features":range(1,11,1)}
gsearch1 = GridSearchCV(estimator=RandomForestClassifier(n_estimators=81,
                        random_state=10),
                        param_grid = param_test2,scoring='roc_auc',cv=10)
gsearch1.fit(X,y)
print(gsearch1.grid_scores_)
print(gsearch1.best_params_)
print('best accuracy:%f' % gsearch1.best_score_)

```











#### 一.调节随机森林算法

<img src="/Users/zhuhe/Library/Application Support/typora-user-images/image-20220528203245119.png" alt="image-20220528203245119" style="zoom:33%;" />



#### RF框架参数意义

##### 1.n_estimators:

###### 对原始数据集进行有放回抽样生成的子数据集个数，即[决策树](https://www.zhihu.com/search?q=决策树&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"56940098"})的个数。

版本0.20的默认值是10,版本0.22的默认值是100。

###### 2.bootstrap:

是否对样本集进行有放回抽样来构建树，True表示是,默认值True。

###### 4.oob_score:

是否采用袋外样本来评估模型的好坏，True代表是，默认值False,袋外样本误差是测试数据集误差的无偏估计，所以推荐设置True。



#### RF决策树参数含义

##### 1.max_features:

构建决策树最优模型时考虑的最大特征数。默认是"auto"，表示最大特征数是N的平方根;“log2"表示最大特征数是 ![[公式]](https://www.zhihu.com/equation?tex=log_%7B2%7DN) ;"sqrt"表示最大特征数是 ![[公式]](https://www.zhihu.com/equation?tex=%5Csqrt%7BN%7D) 。如果是整数，代表考虑的最大特征数；如果是浮点数，表示对(N * max_features)取整。其中N表示样本的特征数。

##### 2.max_depth:

决策树最大深度。若等于None,表示决策树在构建最优模型的时候不会限制子树的深度。如果模型样本量多，特征也多的情况下，推荐限制最大深度；若样本量少或者特征少，则不限制最大深度。

##### 3.criterion:

表示节点的划分标准。不纯度标准参考Gini指数，信息增益标准参考"entrop"熵。



##### 接下来的参数都和剪枝有关系：（让模型的复杂度降低）

4.min_samples_leaf:叶子节点含有的最少样本。若叶子节点样本数小于min_samples_leaf，则对该叶子节点和兄弟叶子节点进行剪枝，只留下该叶子节点的父节点。整数型表示个数，浮点型表示取大于等于（样本数 * min_samples_leaf)的最小整数。min_samples_leaf默认值是1。

5.min_samples_split:节点可分的最小样本数，默认值是2。整数型和浮点型的含义与min_samples_leaf类似。

6.max_leaf_nodes:最大叶子节点数。int设置节点数,None表示对叶子节点数没有限制。

7.min_impurity_decrease:

节点划分的最小不纯度。假设不纯度用信息增益表示，若某节点划分时的信息增益大于等于min_impurity_decrease，那么该节点还可以再划分；反之，则不能划分。

8.min_samples_leaf:叶子节点最小的样本权重和。叶子节点如果小于这个值，则会和兄弟节点一起被剪枝，只保留叶子节点的父节点。默认是0，则不考虑样本权重问题。一般来说，如果有较多样本的缺失值或偏差很大，则尝试设置该参数值。



随机森林算法的优点：

1.模型的随机性很强；不容易overfitting；对outlier不敏感

2.处理高维度的数据更加快速

3.树状结构，模型的可解释性更强；

##### 缺点：天花板太低了



```python
from sklearn.datasets import load_breast_cancer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 导入乳腺癌数据集
data = load_breast_cancer()

# 建立随机森林
rfc = RandomForestClassifier(n_estimators=100, random_state=90)

score_pre = cross_val_score(rfc, data.data, data.target, cv=10).mean()
score_pre
```







###### 1.首先调节n_estimators：（根据复杂度）

```python
# 调参，绘制学习曲线来调参n_estimators（对随机森林影响最大）
score_lt = []

# 每隔10步建立一个随机森林，获得不同n_estimators的得分
for i in range(0,250,10):
    rfc = RandomForestClassifier(n_estimators=i+1
                                 ,random_state=90)
    score = cross_val_score(rfc, data.data, data.target, cv=10).mean()
    score_lt.append(score)
score_max = max(score_lt)
print('最大得分：{}'.format(score_max),
      '子树数量为：{}'.format(score_lt.index(score_max)*10+1))

# 绘制学习曲线
x = np.arange(1,251,10)
plt.subplot(111)
plt.plot(x, score_lt, 'r-')
plt.show()


score_lt = []
for i in range(65,80):
    rfc = RandomForestClassifier(n_estimators=i,random_state=90)
    score = cross_val_score(rfc, data.data, data.target, cv=10).mean()
    score_lt.append(score)
score_max = max(score_lt)
print('最大得分：{}'.format(score_max),
      '子树数量为：{}'.format(score_lt.index(score_max)+65))

# 绘制学习曲线
x = np.arange(65,80)
plt.subplot(111)
plt.plot(x, score_lt,'o-')
plt.show()
```

<img src="/Users/zhuhe/Library/Application Support/typora-user-images/image-20220528203138519.png" alt="image-20220528203138519" style="zoom:50%;" />

###### 2.调节max_depth：（根据复杂度）

```python
# 建立n_estimators为45的随机森林
rfc = RandomForestClassifier(n_estimators=45, random_state=90)

# 用网格搜索调整max_depth
param_grid = {'max_depth':np.arange(1,20)}
GS = GridSearchCV(rfc, param_grid, cv=10)
GS.fit(data.data, data.target)

best_param = GS.best_params_
best_score = GS.best_score_
print(best_param, best_score)
```

<img src="/Users/zhuhe/Library/Application Support/typora-user-images/image-20220528203150501.png" alt="image-20220528203150501" style="zoom:50%;" />

###### 3.max_features:

```python
# 用网格搜索调整max_features
param_grid = {'max_features':np.arange(5,31)}

rfc = RandomForestClassifier(n_estimators=45
                             ,random_state=90
                             ,max_depth=11)
GS = GridSearchCV(rfc, param_grid, cv=10)
GS.fit(data.data, data.target)
best_param = GS.best_params_
best_score = GS.best_score_
print(best_param, best_score)
```

<img src="/Users/zhuhe/Library/Application Support/typora-user-images/image-20220528203202198.png" alt="image-20220528203202198" style="zoom:50%;" />





首先增大n_estimators,提高模型的拟合能力，当模型的拟合能力没有明显提升的时候，则在增大max_features，提高每个子模型的拟合能力，则相应的提高了模型的拟合能力。上面的参数调优是一种比较常用的调优方法，可以应用到其他模型的参数优化过程中。





#### 二.调节XGboosting

https://www.zhihu.com/search?type=content&q=XGBRegressor

<img src="/Users/zhuhe/Library/Application Support/typora-user-images/image-20220530015103014.png" alt="image-20220530015103014" style="zoom:50%;" />



<img src="/Users/zhuhe/Library/Application Support/typora-user-images/image-20220530015148565.png" alt="image-20220530015148565" style="zoom:50%;" />













